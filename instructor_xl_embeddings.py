#!/usr/bin/env python3
"""
Instructor-XL + Transformers åŸ‹ã‚è¾¼ã¿å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
OpenAI text-embedding-3-small ã‹ã‚‰ã®ç§»è¡Œç”¨
"""

import torch
from sentence_transformers import SentenceTransformer
import psycopg2
import numpy as np
from typing import List, Dict, Any, Optional
import json
import logging
from datetime import datetime
import os
from dataclasses import dataclass

@dataclass
class EmbeddingConfig:
    model_name: str = "hkunlp/instructor-xl"
    max_length: int = 512  # Instructor-XLã®æ¨å¥¨æœ€å¤§é•·
    batch_size: int = 8
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
@dataclass
class DatabaseConfig:
    host: str = "localhost"
    port: int = 5432
    database: str = "postgres"
    user: str = "postgres"
    password: str = ""  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—æ¨å¥¨

class InstructorXLEmbedder:
    """Instructor-XLåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.logger = self._setup_logger()
        self.model = self._load_model()
        
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger("instructor_xl_embedder")
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger
    
    def _load_model(self) -> SentenceTransformer:
        """Instructor-XLãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        self.logger.info(f"ğŸ§  Loading Instructor-XL model: {self.config.model_name}")
        
        try:
            model = SentenceTransformer(self.config.model_name)
            model = model.to(self.config.device)
            
            self.logger.info(f"âœ… Model loaded on device: {self.config.device}")
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ Model loading failed: {e}")
            raise
    
    def _chunk_text(self, text: str, max_length: int = None) -> List[str]:
        """é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªé•·ã•ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        if max_length is None:
            max_length = self.config.max_length
            
        # æ–‡åŒºåˆ‡ã‚Šã§ã®åˆ†å‰²ã‚’å„ªå…ˆ
        sentences = text.split('ã€‚')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # ãƒãƒ£ãƒ³ã‚¯é•·åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(current_chunk + sentence + 'ã€‚') <= max_length:
                current_chunk += sentence + 'ã€‚'
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + 'ã€‚'
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks if chunks else [text[:max_length]]
    
    def generate_embeddings(
        self, 
        texts: List[str], 
        instruction: str = "Represent the UI component for retrieval"
    ) -> List[np.ndarray]:
        """Instructor-XLã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆinstructionä»˜ãï¼‰"""
        
        # Instructionã‚’è¿½åŠ ã—ãŸãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
        instructed_texts = []
        for text in texts:
            # é•·æ–‡ã®å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._chunk_text(text)
            for chunk in chunks:
                instructed_texts.append([instruction, chunk])
        
        self.logger.info(f"ğŸ”¢ Generating embeddings for {len(instructed_texts)} text chunks")
        
        try:
            embeddings = self.model.encode(
                instructed_texts,
                batch_size=self.config.batch_size,
                show_progress_bar=True,
                convert_to_numpy=True
            )
            
            # ãƒãƒ£ãƒ³ã‚¯ãŒè¤‡æ•°ã®å ´åˆã¯å¹³å‡åŒ–
            if len(instructed_texts) > len(texts):
                # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’å¹³å‡åŒ–ã—ã¦å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆæ•°ã«åˆã‚ã›ã‚‹
                result_embeddings = []
                chunks_per_text = len(instructed_texts) // len(texts)
                
                for i in range(len(texts)):
                    start_idx = i * chunks_per_text
                    end_idx = (i + 1) * chunks_per_text
                    chunk_embeddings = embeddings[start_idx:end_idx]
                    averaged_embedding = np.mean(chunk_embeddings, axis=0)
                    result_embeddings.append(averaged_embedding)
                    
                embeddings = np.array(result_embeddings)
            
            self.logger.info(f"âœ… Generated embeddings with shape: {embeddings.shape}")
            return embeddings.tolist()
            
        except Exception as e:
            self.logger.error(f"âŒ Embedding generation failed: {e}")
            raise

class RAGDocumentProcessor:
    """RAGãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ»ä¿å­˜ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_config: DatabaseConfig, embedder: InstructorXLEmbedder):
        self.db_config = db_config
        self.embedder = embedder
        self.logger = embedder.logger
        
    def get_db_connection(self):
        """PostgreSQLæ¥ç¶š"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.user,
                password=self.db_config.password
            )
            return conn
        except Exception as e:
            self.logger.error(f"âŒ Database connection failed: {e}")
            raise
    
    def process_document(
        self, 
        title: str,
        ui_type: str,
        description: str,
        content: str,
        keywords: List[str] = None,
        source_url: str = None,
        paste_context: Dict[str, Any] = None,
        claude_evaluation: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ã¨åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        
        self.logger.info(f"ğŸ“‹ Processing document: {title}")
        
        # 3ç¨®é¡ã®ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
        main_text = f"{title} {description} {' '.join(keywords or [])}"
        content_text = content
        title_text = title
        
        # å„ã‚¿ã‚¤ãƒ—åˆ¥ã®instruction
        instructions = {
            "main": "Represent the UI component description for semantic search",
            "content": "Represent the UI component code/markup for technical retrieval", 
            "title": "Represent the UI component title for quick identification"
        }
        
        try:
            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            main_embedding = self.embedder.generate_embeddings(
                [main_text], instructions["main"]
            )[0]
            
            content_embedding = self.embedder.generate_embeddings(
                [content_text], instructions["content"]  
            )[0]
            
            title_embedding = self.embedder.generate_embeddings(
                [title_text], instructions["title"]
            )[0]
            
            # Claudeè©•ä¾¡ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            if claude_evaluation is None:
                claude_evaluation = {
                    "consistency_score": 0.8,
                    "quality": {
                        "reusability": "ä¸­",
                        "maintainability": "ä¸­",
                        "accessibility": "ä¸­"
                    },
                    "improvements": [],
                    "ui_classification": {
                        "primary_type": ui_type,
                        "secondary_types": []
                    }
                }
            
            return {
                "title": title,
                "ui_type": ui_type,
                "description": description,
                "copied_content": content,
                "keywords": keywords or [],
                "source_url": source_url,
                "paste_context": paste_context or {},
                "claude_evaluation": claude_evaluation,
                "evaluation_score": claude_evaluation.get("consistency_score", 0.8),
                "improvement_notes": claude_evaluation.get("improvements", []),
                "embedding": main_embedding,
                "content_embedding": content_embedding,
                "title_embedding": title_embedding,
                "embedding_model": "instructor-xl",
                "embedding_generated_at": datetime.now().isoformat(),
                "is_approved": True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Document processing failed for '{title}': {e}")
            raise
    
    def save_to_database(self, document_data: Dict[str, Any]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜"""
        
        insert_query = """
        INSERT INTO rag_documents_instructor (
            title, ui_type, description, copied_content, keywords, source_url,
            paste_context, claude_evaluation, evaluation_score, improvement_notes,
            embedding, content_embedding, title_embedding,
            embedding_model, embedding_generated_at, is_approved
        ) VALUES (
            %(title)s, %(ui_type)s, %(description)s, %(copied_content)s, 
            %(keywords)s, %(source_url)s, %(paste_context)s,
            %(claude_evaluation)s, %(evaluation_score)s, %(improvement_notes)s,
            %(embedding)s, %(content_embedding)s, %(title_embedding)s,
            %(embedding_model)s, %(embedding_generated_at)s, %(is_approved)s
        ) RETURNING id;
        """
        
        try:
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    # JOSNBãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                    save_data = document_data.copy()
                    save_data['paste_context'] = json.dumps(save_data['paste_context'])
                    save_data['claude_evaluation'] = json.dumps(save_data['claude_evaluation'])
                    
                    cur.execute(insert_query, save_data)
                    doc_id = cur.fetchone()[0]
                    conn.commit()
                    
            self.logger.info(f"âœ… Document saved with ID: {doc_id}")
            return doc_id
            
        except Exception as e:
            self.logger.error(f"âŒ Database save failed: {e}")
            raise

class RAGQuerySearcher:
    """RAGã‚¯ã‚¨ãƒªæ¤œç´¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_config: DatabaseConfig, embedder: InstructorXLEmbedder):
        self.db_config = db_config
        self.embedder = embedder
        self.logger = embedder.logger
    
    def get_db_connection(self):
        """PostgreSQLæ¥ç¶š"""
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.user,
                password=self.db_config.password
            )
            return conn
        except Exception as e:
            self.logger.error(f"âŒ Database connection failed: {e}")
            raise
    
    def search_similar_documents(
        self,
        query: str,
        limit: int = 5,
        min_similarity: float = 0.7
    ) -> List[Dict[str, Any]]:
        """é¡ä¼¼åº¦æ¤œç´¢"""
        
        self.logger.info(f"ğŸ” Searching for: '{query}'")
        
        try:
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_embedding = self.embedder.generate_embeddings(
                [query], 
                "Represent the search query for finding relevant UI components"
            )[0]
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
            with self.get_db_connection() as conn:
                with conn.cursor() as cur:
                    search_query = """
                    SELECT * FROM search_rag_instructor(%s, %s, %s);
                    """
                    
                    cur.execute(search_query, (query_embedding, limit, min_similarity))
                    results = cur.fetchall()
                    
                    # çµæœã®æ•´å½¢
                    columns = [desc[0] for desc in cur.description]
                    search_results = []
                    
                    for row in results:
                        result_dict = dict(zip(columns, row))
                        search_results.append(result_dict)
            
            self.logger.info(f"ğŸ¯ Found {len(search_results)} similar documents")
            return search_results
            
        except Exception as e:
            self.logger.error(f"âŒ Search failed: {e}")
            raise
    
    def format_for_claude(
        self,
        search_results: List[Dict[str, Any]],
        original_query: str
    ) -> Dict[str, str]:
        """Claudeå‘ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•´å½¢"""
        
        system_prompt = """ã‚ãªãŸã¯UI/UXã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

æ¤œç´¢ã•ã‚ŒãŸUIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:"""
        
        user_prompt = f"è³ªå•: {original_query}\n\nå‚è€ƒæƒ…å ±:\n"
        
        for i, result in enumerate(search_results, 1):
            system_prompt += f"""

{i}. {result['title']} (é¡ä¼¼åº¦: {result['similarity']:.2f})
   - UIã‚¿ã‚¤ãƒ—: {result['ui_type']}
   - èª¬æ˜: {result['description']}
   - è©•ä¾¡ã‚¹ã‚³ã‚¢: {result['evaluation_score']:.2f}
"""
            
            if result.get('claude_evaluation'):
                eval_data = json.loads(result['claude_evaluation']) if isinstance(result['claude_evaluation'], str) else result['claude_evaluation']
                improvements = eval_data.get('improvements', [])
                if improvements:
                    system_prompt += f"   - æ”¹å–„ææ¡ˆ: {', '.join(improvements)}\n"
        
        user_prompt += "\nä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«å¯¾ã™ã‚‹å…·ä½“çš„ã§å®Ÿç”¨çš„ãªå›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"
        
        return {
            "system": system_prompt,
            "user": user_prompt
        }

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ã¨ãƒ†ã‚¹ãƒˆ"""
    
    # è¨­å®š
    embedding_config = EmbeddingConfig()
    db_config = DatabaseConfig(
        password=os.getenv("POSTGRES_PASSWORD", "your_password")
    )
    
    # åˆæœŸåŒ–
    embedder = InstructorXLEmbedder(embedding_config)
    processor = RAGDocumentProcessor(db_config, embedder)
    searcher = RAGQuerySearcher(db_config, embedder)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    sample_docs = [
        {
            "title": "Bootstrap Card Component",
            "ui_type": "card",
            "description": "ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãªã‚«ãƒ¼ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€‚ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ã‚’å«ã‚€",
            "content": """<div class="card" style="width: 18rem;">
                <img src="..." class="card-img-top" alt="...">
                <div class="card-body">
                    <h5 class="card-title">Card title</h5>
                    <p class="card-text">Some quick example text</p>
                    <a href="#" class="btn btn-primary">Go somewhere</a>
                </div>
            </div>""",
            "keywords": ["Bootstrap", "ã‚«ãƒ¼ãƒ‰", "ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–", "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"],
            "source_url": "https://getbootstrap.com/docs/5.0/components/card/"
        },
        {
            "title": "Material-UI Navigation Drawer",
            "ui_type": "navigation",
            "description": "ã‚µã‚¤ãƒ‰ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‰ãƒ­ãƒ¯ãƒ¼ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ",
            "content": """<Drawer
                variant="temporary"
                anchor="left"
                open={open}
                onClose={handleClose}
            >
                <List>
                    <ListItem button>
                        <ListItemText primary="Home" />
                    </ListItem>
                    <ListItem button>
                        <ListItemText primary="Profile" />
                    </ListItem>
                </List>
            </Drawer>""",
            "keywords": ["Material-UI", "ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³", "ãƒ‰ãƒ­ãƒ¯ãƒ¼", "React"],
            "source_url": "https://mui.com/components/drawers/"
        }
    ]
    
    print("ğŸš€ Instructor-XL RAG ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ»ä¿å­˜
    print("\nğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ä¸­...")
    for doc in sample_docs:
        try:
            processed_doc = processor.process_document(**doc)
            doc_id = processor.save_to_database(processed_doc)
            print(f"âœ… Successfully processed: {doc['title']} (ID: {doc_id})")
            
        except Exception as e:
            print(f"âŒ Failed to process: {doc['title']} - {e}")
    
    # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\nğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆä¸­...")
    test_queries = [
        "ã‚«ãƒ¼ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å®Ÿè£…æ–¹æ³•",
        "ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ä½œã‚Šæ–¹",
        "ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãªUIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"
    ]
    
    for query in test_queries:
        try:
            results = searcher.search_similar_documents(query, limit=3)
            claude_prompt = searcher.format_for_claude(results, query)
            
            print(f"\nğŸ“ Query: {query}")
            print(f"ğŸ¯ Found {len(results)} results")
            
            if results:
                print("Claudeç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
                print(f"System: {claude_prompt['system'][:200]}...")
                print(f"User: {claude_prompt['user'][:100]}...")
            
        except Exception as e:
            print(f"âŒ Search failed for '{query}': {e}")

if __name__ == "__main__":
    main()